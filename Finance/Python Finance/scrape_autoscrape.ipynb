{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(): # Learning\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "        \n",
    "    replace_target_network()\n",
    "\n",
    "    states, actions, rewards, states_, dones = sample_memory()\n",
    "        \n",
    "    states = T.tensor(states,dtype=T.float)\n",
    "    actions = T.tensor(actions,dtype=T.long)\n",
    "    rewards = T.tensor(rewards,dtype=T.float)\n",
    "    states_ = T.tensor(states_,dtype=T.float)\n",
    "    dones = T.tensor(dones,dtype=T.float)\n",
    "        \n",
    "    indices = np.arange(batch_size)\n",
    "        \n",
    "\n",
    "        \n",
    "    for epoch in range(self.epochs):\n",
    "        optim.zero_grad() # Setting the optimizer and Zeroing the gradients before every step\n",
    "            \n",
    "        q_pred = q.fp(states) # Predected Q Value at current state\n",
    "        q_next = q_next.fp(states_).max(dim=1)#(dim=1)[0] # Predected Q value at next state\n",
    "            \n",
    "          \n",
    "        dones = T.tensor(dones.numpy(),dtype=T.float)\n",
    "        q_next = q_next.values*(1-dones)\n",
    "\n",
    "        q_target = rewards + self.gamma*q_next \n",
    "            \n",
    "        loss = q.loss(q_target, q_pred[bids,actions]) # Temporal Diferrence (TD Loss)\n",
    "\n",
    "        loss.backward() # Back Propogate Loss\n",
    "        optim.step()\n",
    "        step += 1 # Update the step counter\n",
    "\n",
    "    if eps > eps_min: # Update the epsilon \n",
    "        eps *= decay\n",
    "    \n",
    "def play_episode():\n",
    "    se = np.random.randint(0,100000,1)[0]\n",
    "    env = env.seed(se)\n",
    "    done = False\n",
    "    r = 0\n",
    "    episode += 1\n",
    "    obs = T.tensor(env.reset().flatten(),dtype=T.float)\n",
    "    while not done:\n",
    "            action = int(self.get_action(obs))\n",
    "            next_state,reward,done,_ = self.env.step(action)\n",
    "            next_state = T.tensor(next_state.flatten(),dtype=T.float)\n",
    "            \n",
    "            self.store_transition(obs, action,reward, next_state, done)\n",
    "            r += reward\n",
    "            self.learn()\n",
    "            obs = next_state\n",
    "        return r\n",
    "            \n",
    "    \n",
    "    def train(self,hours):\n",
    "        seconds = 3600*hours\n",
    "        start_time = datetime.now()\n",
    "        end_time = datetime.now() + timedelta(seconds=seconds)\n",
    "        R = []\n",
    "        \n",
    "        while datetime.now() < end_time:\n",
    "            self.writer.add_scalar('Epsilon',self.eps,self.episode)\n",
    "            episode_reward = self.play_episode()\n",
    "            self.writer.add_scalar('Episode Reward',episode_reward,self.episode)\n",
    "            R.append(episode_reward)\n",
    "            if self.episode % 5 == 0:\n",
    "                print(self.episode)\n",
    "                print('mean of R : ',np.mean(R))\n",
    "                print('last 100 games mean :', np.mean(R[-100:]))\n",
    "                print(self.eps,' epsilon')\n",
    "                plt.plot(list(range(len(R))),R)\n",
    "                plt.show()\n",
    "            \n",
    "        return self.q.state_dict()\n",
    "    \n",
    "    def play_vehicle_ahed(self):\n",
    "        seed = 0\n",
    "        act = [1 for _ in range(5)]\n",
    "        self.env.seed(seed)\n",
    "        r = 0\n",
    "        for i in Act:\n",
    "            obs,reward,done,_ = env.step(i)\n",
    "            \n",
    "        prt = 'Situation reached where action 1 is bad - car ahed'\n",
    "        data = [self.eps]\n",
    "        obs = T.tensor(obs.flatten(),dtype = T.float)\n",
    "        qvals = self.q.fp(obs)\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            action = int(self.get_action(obs))\n",
    "            next_state,reward,done,_ = self.env.step(action)\n",
    "            next_state = T.tensor(next_state.flatten(),dtype=T.float)\n",
    "            \n",
    "            self.store_transition(obs, action,reward, next_state, done)\n",
    "            r += reward\n",
    "            obs = next_state\n",
    "        return r\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
